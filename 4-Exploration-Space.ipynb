{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Synthetic Promoter Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook generates a synthetic promoter library for *P. putida* on the basis of promoter BG14g ([Zobel et al., 2015](https://doi.org/10.1021/acssynbio.5b00058)). The synthetic promoters are generated within the distance that has been experimentaly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initiation\n",
    "\n",
    "Loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ExpressionExpert_Functions import Data_Src_Load, Sequence_Conserved_Adjusted, Insert_row_, ExpressionStrength_HeatMap, SequenceRandomizer_Parallel, Sequence_Ref_DiffSum, list_onehot, list_integer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable setting\n",
    "\n",
    "We load the naming conventions from 'config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Dict = dict()\n",
    "with open('config_Spacer.txt') as Conf:\n",
    "    myline = Conf.read().splitlines()\n",
    "    for line in myline:\n",
    "        if not line.startswith('#'):\n",
    "            (key, val) = line.split(':', 1)\n",
    "            Name_Dict[str(key.strip())] = val.strip()\n",
    "        \n",
    "\n",
    "Data_File = Name_Dict['Data_File']\n",
    "# extract the filename for naming of newly generated files\n",
    "File_Base = Name_Dict['File_Base']\n",
    "# the generated files will be stored in a subfolder with custom name\n",
    "Data_Folder = Name_Dict['Data_Folder']\n",
    "# column name of expression values\n",
    "Y_Col_Name = eval(Name_Dict['Y_Col_Name'])\n",
    "# Extracting entropy cutoff for removal of non-informative positions\n",
    "Entropy_cutoff = float(Name_Dict['Entropy_cutoff'])\n",
    "# figure file type\n",
    "Fig_Type = Name_Dict['Figure_Type']\n",
    "# Figure font size\n",
    "FigFontSize = Name_Dict['Figure_Font_Size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "General information on the data source csv-file is stored in the 'config.txt' file generated in the '0-Workflow' notebook. The sequence and expression data is stored in a csv file with an identifier in column 'ID' (not used for anything), the DNA-sequence in column 'Sequence', and the expression strength in column 'promoter activity'. While loading, the sequence is converted to a label encrypted sequence, ['A','C','G','T'] replaced by [0,1,2,3], and a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqDat = Data_Src_Load(Name_Dict)\n",
    "SeqDat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting of exploration boundaries\n",
    "\n",
    "### Extraction of experimentaly tested sequence positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing non-informative positions where no base diversity exists, base one hot encoding\n",
    "SeqDat_Hadj, Positions_removed, PSEntropy = Sequence_Conserved_Adjusted(SeqDat, Name_Dict, Entropy_cutoff=Entropy_cutoff)\n",
    "\n",
    "# removing non-informative positions where no base diversity exists, based one hot encoding\n",
    "idx = 0\n",
    "Measure_Name = Y_Col_Name[idx]\n",
    "# Expression_Column = '{}_scaled'.format(Measure_Name)\n",
    "Expr_avg = ExpressionStrength_HeatMap(SeqDat_Hadj, Measure_Name)\n",
    "Expr_avg = Insert_row_(Positions_removed, Expr_avg, np.zeros([len(Positions_removed),4])*np.nan)\n",
    "Seq_Pos_Sampled = Expr_avg.notnull().astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the positions for nucleotide exchanges are determined\n",
    "# The maximum sequence distance for experimentaly teste sequences to the reference was four nucleotides (cf. histogram of sequence distance in the statistical analysis notebook)\n",
    "# We require an minimum positional entropy of 0.15 bit to assume sufficient sampling for reasonable prediction accuracy (cf. position entropy in the statistical analysis notebook)\n",
    "# The random forest machine learner has assigned reasonable feature importance only to regions [-35 - -30] and [-12 - -7]\n",
    "# Hence, mutations are only assigned to positions [-35, -34, -30, -10, -9, -8]\n",
    "# For each position the following amount of nucleotides were tested [-35:4, -34:4, -30:3, -10:4, -9:3, -8:4]\n",
    "# In total 2304 combinations are possible, these are further constrained by the maximum nucleotide distance to the reference sequence\n",
    "\n",
    "Sequence_Distance_cutoff = float(Name_Dict['Sequence_Distance_cutoff'])\n",
    "Synth_Seq_MaxNumber = int(Name_Dict['Synth_Seq_MaxNumber'])\n",
    "\n",
    "# Reference promoter sequence\n",
    "if Name_Dict['RefSeq'] is not '':\n",
    "    RefSeq = Name_Dict['RefSeq']\n",
    "    print('use reference sequence')\n",
    "else:    \n",
    "    # using the one-hot encoding the most common nucleotide on each position is calculated.\n",
    "    Alphabet = ['A','C','G','T']\n",
    "    Pos_Nucl_Sum = np.sum(np.dstack(SeqDat['Sequence'].values), axis=2)\n",
    "    RefSeq_list = list([Alphabet[Pos_idx] for Pos_idx in np.argmax(Pos_Nucl_Sum, axis=1)])\n",
    "    RefSeq = ''.join(RefSeq_list)\n",
    "print('Reference sequence:', RefSeq)\n",
    "\n",
    "# Nucleotides that can be randomized because they sampling is above the entropy threshold\n",
    "Pos_random = -1*(len(RefSeq)-np.arange(PSEntropy.shape[0])[PSEntropy>Entropy_cutoff])\n",
    "Base_SequencePosition = Seq_Pos_Sampled.iloc[Pos_random]\n",
    "print('Relevant positions and tested nucleotides')\n",
    "print(Base_SequencePosition)\n",
    "# calculating how many different sequences exist\n",
    "# we check how many bases for each sufficiently sampled position are possible\n",
    "# combinatorics gives the final answer\n",
    "mybases, mycount = np.unique(np.sum(Base_SequencePosition.values, axis=1), return_counts=True)\n",
    "Comb_Numb = np.prod(np.power(mybases, mycount))\n",
    "print('Overall number of possible sequences:', Comb_Numb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the space of possible sequences is lower than the maximum limit space, each sequences is generated\n",
    "# if the space of possible sequences is larger than the maximum limit space, random sequences within the exploratory space are generated\n",
    "\n",
    "if Comb_Numb < Synth_Seq_MaxNumber:\n",
    "    # Deletion of non-tested nucleotides\n",
    "    Pos_rand_numb = len(Pos_random)\n",
    "    Alphabet = ['A','C','G','T']\n",
    "    Seq_Base = np.tile(Alphabet, [Pos_rand_numb, 1]).tolist()\n",
    "    # identification of positions where not all four nucleotides were tested\n",
    "    Pos_Del, Nucl_Del = np.where(Seq_Pos_Sampled.iloc[Pos_random].values == 0)\n",
    "    # replacing all nucleotides to be replaced by an 'X'\n",
    "    myArr = np.array(Seq_Base)\n",
    "    myArr[tuple([Pos_Del,Nucl_Del])] = 'X'\n",
    "    Position_list = myArr.tolist()\n",
    "    Seq_Base = list()\n",
    "    for Position in Position_list:\n",
    "        Seq_Base.append(list(set(Position).difference(set('X'))))\n",
    "    Seq_Base_comb = list(itertools.product(*Seq_Base))\n",
    "    for index in Pos_random+40: #+40\n",
    "        RefSeq_list[index] = '{}'\n",
    "    RefSeq_base = ''.join(RefSeq_list)\n",
    "    # setting up the final promoter list\n",
    "    Seq_Base_comb = [RefSeq_base.format(*Nucleotide_replace) for Nucleotide_replace in Seq_Base_comb]\n",
    "\n",
    "    print('generated sequences: ',len(Seq_Base_comb))\n",
    "else:\n",
    "    Seq_Base_comb = SequenceRandomizer_Parallel(RefSeq, Base_SequencePosition, n=Synth_Seq_MaxNumber)\n",
    "    \n",
    "print('Preliminary generation of {} sequences.'.format(len(Seq_Base_comb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring synthetic sequence distance to reference\n",
    "\n",
    "#### Generating arche-type reference\n",
    "The reference is generated as the most commonly tested nucleotides on each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the full promoters\n",
    "# Reference promoter sequence\n",
    "# using the one-hot encoding the most common nucleotide on each position is calculated.\n",
    "\n",
    "# For histogram of sequence diversity you can either root the distance to the most common nucleotide on each position or provide an external reference\n",
    "if Name_Dict['RefSeq'] is not '':\n",
    "    RefSeq = Name_Dict['RefSeq']\n",
    "    print('use reference sequence')\n",
    "else:    \n",
    "    # using the one-hot encoding the most common nucleotide on each position is calculated.\n",
    "    Alphabet = ['A','C','G','T']\n",
    "    Pos_Nucl_Sum = np.sum(np.dstack(SeqDat['Sequence'].values), axis=2)\n",
    "    RefSeq_list = list([Alphabet[Pos_idx] for Pos_idx in np.argmax(Pos_Nucl_Sum, axis=1)])\n",
    "    RefSeq = ''.join(RefSeq_list)\n",
    "\n",
    "print('Reference sequence:', RefSeq)\n",
    "\n",
    "# # RefSeq = SeqDat['Sequence_letter-encrypted'][0]\n",
    "# # RefSeq_list = list(RefSeq) #int(Pos_random+40)\n",
    "# for index in Pos_random+40: #+40\n",
    "#     RefSeq_list[index] = '{}'\n",
    "# RefSeq_base = ''.join(RefSeq_list)\n",
    "# # setting up the final promoter list\n",
    "# SynMatrix_full = [RefSeq_base.format(*Nucleotide_replace) for Nucleotide_replace in Seq_Base_comb]\n",
    "SynMatrix_full = Seq_Base_comb\n",
    "\n",
    "# determining the amino acid substitutions of the snythetic promoters relative to the reference promoter\n",
    "SeqDat_wRef = SynMatrix_full.copy()\n",
    "SeqDat_wRef.insert(0, RefSeq)\n",
    "RefSeq_Dist = Sequence_Ref_DiffSum(SeqDat_wRef)\n",
    "NearDist_Bool = np.array(RefSeq_Dist)<Sequence_Distance_cutoff\n",
    "\n",
    "RefSeq_NearDist = RefSeq_Dist[NearDist_Bool]\n",
    "SynSeq_NearDist = np.array(SynMatrix_full)[NearDist_Bool]\n",
    "SynSeq_ND_numb = SynSeq_NearDist.shape[0]\n",
    "print('Number of sequences with less than {:.0f}% nucleotide changes to the reference: {}'.format(Sequence_Distance_cutoff*100, SynSeq_ND_numb))\n",
    "\n",
    "plt.hist(RefSeq_NearDist*100) # BG42: [3,6:221]; BG35:[2,222:]\n",
    "plt.xlabel('Sequence distance in %')\n",
    "plt.ylabel('Occurence')\n",
    "plt.tight_layout()\n",
    "Fig_ID = Name_Dict['SampSeqDist_File']\n",
    "SampSeqDist_File = os.path.join(Data_Folder, '{}_{}_{}.{}'.format(time.strftime('%Y%m%d'), File_Base, Fig_ID, Fig_Type))\n",
    "# plt.savefig(SampSeqDist_File, bbox_inches='tight', format=Fig_Type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing measured sequences from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq_Measured = np.unique(SeqDat['Sequence_letter-encrypted'].values)\n",
    "print('Number of different promoter sequences measured: ', Seq_Measured.shape[0])\n",
    "\n",
    "MeasSeq_Idx = [list(np.arange(SynSeq_ND_numb)[SynSeq_NearDist == np.array(SeqMeas.upper())]) for SeqMeas in Seq_Measured]\n",
    "MeasSeq_Idx = np.unique(np.asarray(list(filter(None, MeasSeq_Idx))))\n",
    "print('Library sequences already measured: ', MeasSeq_Idx)\n",
    "# Some sequences are often repeatedly measured in the experiments, e.g. sequence 1116 in 'SynSeq_NearDist'\n",
    "# print('Sequence of synthetic sequence 1116 measured 93 times', SynSeq_NearDist[1116])\n",
    "\n",
    "# deletion of measured sequences\n",
    "SynSeq_NearDist = np.delete(SynSeq_NearDist, MeasSeq_Idx)\n",
    "print('Number of new sequences in exploratory region: ', SynSeq_NearDist.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic library expression strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Type = Name_Dict['ML_Regressor']\n",
    "ML_Date = Name_Dict['ML_Date']\n",
    "Sequence_column = Name_Dict['Sequence_column']\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "SynSeq_df = pd.DataFrame({Sequence_column: SynSeq_NearDist})\n",
    "# GC content calculation\n",
    "AddFeat = eval(Name_Dict['Add_Feat'])[0]\n",
    "GCcont = [(SynSeq_df[Sequence_column][i].count('G')+SynSeq_df[Sequence_column][i].count('C'))/len(SynSeq_df[Sequence_column][i]) for i in range(len(SynSeq_df))]\n",
    "SynSeq_df[AddFeat] = GCcont\n",
    "\n",
    "for Meas_Idx in range(Measure_Numb):\n",
    "    print('Prediction of', Y_Col_Name[Meas_Idx])\n",
    "    \n",
    "    # loading the random forest model\n",
    "    Regressor_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Regressor.pkl'.format(ML_Date, File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Type))\n",
    "    ML_Best = joblib.load(Regressor_File)\n",
    "    \n",
    "    # loading data preparation parameters\n",
    "    Parameter_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Params.pkl'.format(ML_Date, File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Type))\n",
    "    Data_Prep_Params = pickle.load(open(Parameter_File, 'rb'))\n",
    "    \n",
    "    Positions_removed = Data_Prep_Params['Positions_removed']\n",
    "    # if the data was standardized we load the corresponding function\n",
    "    if eval(Name_Dict['Data_Standard']):\n",
    "        Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "        Expr_Scaler = Data_Prep_Params[Scaler_DictName]\n",
    "    \n",
    "    # prediction of expression strength\n",
    "    n = len(SynSeq_NearDist)\n",
    "    # one-hot encoded input with noninformative positions removed\n",
    "    X_Test = np.array(list_onehot(np.delete(list_integer(SynSeq_NearDist),Positions_removed, axis=1))).reshape(n,-1)  \n",
    "    # adding the additional feature, here GC-content\n",
    "    X_Test = np.append(X_Test,SynSeq_df[AddFeat].values.reshape(-1,1), axis=1)\n",
    "    Y_Test = ML_Best.predict(X_Test)\n",
    "    # if the data was standardized we inverse transform to get original activity\n",
    "    if eval(Name_Dict['Data_Standard']):\n",
    "        Y_Test = Expr_Scaler.inverse_transform(Y_Test)\n",
    "    SynSeq_df[Y_Col_Name[Meas_Idx]] = Y_Test\n",
    "\n",
    "SynSeq_df = SynSeq_df.sort_values(by=Y_Col_Name[0])    \n",
    "Csv_ID = Name_Dict['Csv_ID']\n",
    "SynCsv_File = os.path.join('{}_{}_{}.csv'.format(time.strftime('%Y%m%d'), File_Base, Csv_ID)) #'data-PromLib_EcolPtai\\\\TillTest_predicted.xlsx'     \n",
    "SynSeq_df.to_csv(SynCsv_File, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new config file for analysis of synthetic library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the config_synth.txt file\n",
    "Name_Dict['Data_File'] = SynCsv_File\n",
    "with open('config_synth.txt', 'w') as f:\n",
    "    print('# This file contains the naming conventions for all output files. It is automatically generated when going through step \"0-Workflow\".', file=f)\n",
    "    for key, value in Name_Dict.items():\n",
    "        print('{}: {}'.format(key, value), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
